{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "tnf8pbtraEXp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SGcLyzVnXA1Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@funcry/in-depth-understanding-of-attention-mechanism-part-ii-scaled-dot-product-attention-and-its-7743804e610e"
      ],
      "metadata": {
        "id": "AFP2132_acyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    x_max = np.max(x, axis=-1, keepdims=True)\n",
        "    e_x = np.exp(x - x_max)\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "# Numpy implementation\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d = Q.shape[-1]\n",
        "    scores = np.dot(Q, K.T) / np.sqrt(d)\n",
        "    attention_weights = softmax(scores)\n",
        "    output = np.dot(attention_weights, V)\n",
        "    return output, attention_weights\n"
      ],
      "metadata": {
        "id": "9kZzXdbWaGv8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ],
      "metadata": {
        "id": "5qLpjXg-VIya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: https://www.kaggle.com/datasets/shahadhamza/multi30k-dataset\n",
        "\n",
        "\n",
        "*   train.en\n",
        "  *   English sentences to train model on\n",
        "*   train.fr\n",
        "  *   French sentences to train model on\n",
        "*   val.en\n",
        "  *   English sentences for validation\n",
        "*   val.fr\n",
        "  *   French sentences for validation\n",
        "\n"
      ],
      "metadata": {
        "id": "nFKytqiTWrw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementaiton of scaled dot product attention that works with tensorflow\n",
        "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Q, K, V = inputs\n",
        "        d_k = tf.cast(tf.shape(Q)[-1], tf.float32)\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(d_k)\n",
        "        weights = tf.nn.softmax(scores, axis=-1)\n",
        "        output = tf.matmul(weights, V)\n",
        "        return output\n",
        "\n",
        "path = kagglehub.dataset_download(\"shahadhamza/multi30k-dataset\")\n",
        "train_en = Path(os.path.join(path, \"train.en\")).read_text(encoding=\"utf-8\").splitlines()\n",
        "train_fr = Path(os.path.join(path, \"train.fr\")).read_text(encoding=\"utf-8\").splitlines()\n",
        "val_en = Path(os.path.join(path, \"val.en\")).read_text(encoding=\"utf-8\").splitlines()\n",
        "val_fr = Path(os.path.join(path, \"val.fr\")).read_text(encoding=\"utf-8\").splitlines()\n",
        "\n",
        "train_df = pd.DataFrame({\"en\": train_en, \"fr\": train_fr}).iloc[:10000]\n",
        "val_df = pd.DataFrame({\"en\": val_en, \"fr\": val_fr}).iloc[:1000]\n",
        "\n",
        "vocab_size_en = 10000\n",
        "vocab_size_fr = 10000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "max_len = 40\n",
        "\n",
        "text_vectorizer_en = TextVectorization(output_sequence_length=max_len, max_tokens=vocab_size_en)\n",
        "text_vectorizer_fr = TextVectorization(output_sequence_length=max_len, max_tokens=vocab_size_fr)\n",
        "\n",
        "text_vectorizer_en.adapt(train_df[\"en\"])\n",
        "text_vectorizer_fr.adapt(train_df[\"fr\"])\n",
        "\n",
        "X_train = text_vectorizer_en(np.array(train_df[\"en\"]))\n",
        "y_train = text_vectorizer_fr(np.array(train_df[\"fr\"]))\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
        "enc_emb = Embedding(vocab_size_en, embedding_dim, name='encoder_embedding')(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = LSTM(units, return_sequences=True, return_state=True, name='encoder_lstm')(enc_emb)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
        "dec_emb = Embedding(vocab_size_fr, embedding_dim, name='decoder_embedding')(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "decoder_outputs = decoder_lstm(dec_emb, initial_state=[state_h, state_c])[0]\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "attention = ScaledDotProductAttention(name='attention')\n",
        "context = attention([decoder_outputs, encoder_outputs, encoder_outputs])\n",
        "\n",
        "# Concatenate decoder output and context vector\n",
        "combined = Concatenate(name='concatenate')([decoder_outputs, context])\n",
        "output = Dense(vocab_size_fr, activation=\"softmax\", name='output_dense')(combined)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "wKwHIRmobnPr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "outputId": "1cdb7747-6e14-445a-bc57-5fc4dc6c4009"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shahadhamza/multi30k-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.21M/1.21M [00:00<00:00, 77.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,560,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m2,560,000\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m1,574,912\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │                   │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │  \u001b[38;5;34m1,574,912\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m512\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mScaledDotProductA…\u001b[0m │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m1024\u001b[0m)             │            │ attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_dense        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │ \u001b[38;5;34m10,250,000\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m10000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ encoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │ decoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ attention           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ScaledDotProductA…</span> │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │            │ attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_dense        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10,250,000</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,519,824\u001b[0m (70.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,519,824</span> (70.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,519,824\u001b[0m (70.65 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,519,824</span> (70.65 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3"
      ],
      "metadata": {
        "id": "5A9LLhpccVfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target = y_train[:, 1:]\n",
        "decoder_input = y_train[:, :-1]\n",
        "\n",
        "# Convert tensors to NumPy arrays for train_test_split\n",
        "X_train_np = X_train.numpy()\n",
        "decoder_input_np = decoder_input.numpy()\n",
        "decoder_target_np = decoder_target.numpy()\n",
        "\n",
        "X_en_train, X_en_val, X_fr_in_train, X_fr_in_val, X_fr_out_train, X_fr_out_val = train_test_split(\n",
        "    X_train_np, decoder_input_np, decoder_target_np, test_size=0.1, random_state=42)\n",
        "\n",
        "history = model.fit(\n",
        "    [X_en_train, X_fr_in_train],\n",
        "    X_fr_out_train,\n",
        "    validation_data=([X_en_val, X_fr_in_val], X_fr_out_val),\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "def create_inference_models(trained_model):\n",
        "    # Encoder model\n",
        "    encoder_model = Model(trained_model.input[0],\n",
        "                         [trained_model.get_layer('encoder_lstm').output[0],\n",
        "                          trained_model.get_layer('encoder_lstm').output[1],\n",
        "                          trained_model.get_layer('encoder_lstm').output[2]])\n",
        "\n",
        "    # Decoder model inputs\n",
        "    decoder_inputs = Input(shape=(None,), name='decoder_inputs_inf')\n",
        "    decoder_state_input_h = Input(shape=(units,), name='decoder_state_h')\n",
        "    decoder_state_input_c = Input(shape=(units,), name='decoder_state_c')\n",
        "    encoder_outputs_input = Input(shape=(None, units), name='encoder_outputs_inf')\n",
        "\n",
        "    # Decoder layers\n",
        "    dec_emb_inf = trained_model.get_layer('decoder_embedding')(decoder_inputs)\n",
        "    decoder_outputs_inf, state_h_inf, state_c_inf = trained_model.get_layer('decoder_lstm')(\n",
        "        dec_emb_inf, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "    # Attention\n",
        "    attention_output_inf = trained_model.get_layer('attention')(\n",
        "        [decoder_outputs_inf, encoder_outputs_input, encoder_outputs_input])\n",
        "\n",
        "    # Final layers\n",
        "    combined_inf = trained_model.get_layer('concatenate')([decoder_outputs_inf, attention_output_inf])\n",
        "    decoder_outputs_final = trained_model.get_layer('output_dense')(combined_inf)\n",
        "\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs, decoder_state_input_h, decoder_state_input_c, encoder_outputs_input],\n",
        "        [decoder_outputs_final, state_h_inf, state_c_inf])\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "def translate_sentence(encoder_model, decoder_model, input_seq, max_len_decode=40):\n",
        "    # Encode the input sentence\n",
        "    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Initialize decoder input with start token (assuming token 1 is start)\n",
        "    target_seq = np.array([[1]])\n",
        "\n",
        "    decoded_tokens = []\n",
        "\n",
        "    for _ in range(max_len_decode):\n",
        "        # Predict next token\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq, state_h, state_c, encoder_outputs], verbose=0)\n",
        "\n",
        "        # Sample token with highest probability\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Exit if hits padding token or end token\n",
        "        if sampled_token_index == 0:\n",
        "            break\n",
        "\n",
        "        decoded_tokens.append(sampled_token_index)\n",
        "\n",
        "        # Update target sequence and states for next iteration\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "        state_h, state_c = h, c\n",
        "\n",
        "    return decoded_tokens\n",
        "\n",
        "encoder_model, decoder_model = create_inference_models(model)\n",
        "\n",
        "smoothie = SmoothingFunction().method4\n",
        "pred_sentences = []\n",
        "true_sentences = []\n",
        "\n",
        "# Lookup for decoding token IDs to words\n",
        "vocab_fr = text_vectorizer_fr.get_vocabulary()\n",
        "vocab_fr_lookup = dict(enumerate(vocab_fr))\n",
        "\n",
        "# Use validation data for evaluation\n",
        "val_X = text_vectorizer_en(np.array(val_df[\"en\"][:100]))\n",
        "\n",
        "for i in range(10):\n",
        "    input_seq = val_X[i:i+1]\n",
        "\n",
        "    # Generate translation using proper inference\n",
        "    pred_ids = translate_sentence(encoder_model, decoder_model, input_seq)\n",
        "\n",
        "    # Convert token IDs to strings\n",
        "    pred_tokens_raw = []\n",
        "    for tok_id in pred_ids:\n",
        "        # Skip padding tokens\n",
        "        if tok_id != 0:\n",
        "            # Get word, empty string if not found\n",
        "            word = vocab_fr_lookup.get(tok_id, '')\n",
        "            pred_tokens_raw.append(word)\n",
        "\n",
        "    # Remove empty strings and whitespace-only tokens\n",
        "    pred_tokens = []\n",
        "    for token in pred_tokens_raw:\n",
        "        # Check if token has non-whitespace content\n",
        "        if token.strip():\n",
        "            pred_tokens.append(token)\n",
        "\n",
        "    # Get true reference\n",
        "    true_tokens = val_df[\"fr\"].iloc[i].split()\n",
        "\n",
        "    pred_sentences.append(pred_tokens)\n",
        "    true_sentences.append(true_tokens)\n",
        "\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"English: {val_df['en'].iloc[i]}\")\n",
        "    print(f\"True French: {' '.join(true_tokens)}\")\n",
        "    print(f\"Predicted: {' '.join(pred_tokens)}\")\n",
        "    print(\"========================================\")\n",
        "\n",
        "# BLEU calculation\n",
        "bleu_scores = []\n",
        "for i in range(len(true_sentences)):\n",
        "    ref = true_sentences[i]\n",
        "    pred = pred_sentences[i]\n",
        "    if pred:  # Only calculate if we have predictions\n",
        "        bleu = sentence_bleu([ref], pred, smoothing_function=smoothie)\n",
        "        bleu_scores.append(bleu)\n",
        "print(\"Average BLEU Score: \", np.mean(bleu_scores))"
      ],
      "metadata": {
        "id": "br6L4g8hVaEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7671e89-9023-480a-d920-71434fba9c87"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 129ms/step - accuracy: 0.6892 - loss: 2.8720 - val_accuracy: 0.7292 - val_loss: 1.7343\n",
            "Epoch 2/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 126ms/step - accuracy: 0.7328 - loss: 1.6505 - val_accuracy: 0.7545 - val_loss: 1.4496\n",
            "Epoch 3/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 123ms/step - accuracy: 0.7582 - loss: 1.3683 - val_accuracy: 0.7671 - val_loss: 1.3082\n",
            "Epoch 4/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 125ms/step - accuracy: 0.7684 - loss: 1.2226 - val_accuracy: 0.7778 - val_loss: 1.2303\n",
            "Epoch 5/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 129ms/step - accuracy: 0.7787 - loss: 1.1093 - val_accuracy: 0.7823 - val_loss: 1.1840\n",
            "Epoch 6/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 126ms/step - accuracy: 0.7890 - loss: 1.0129 - val_accuracy: 0.7867 - val_loss: 1.1509\n",
            "Epoch 7/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 131ms/step - accuracy: 0.7959 - loss: 0.9368 - val_accuracy: 0.7927 - val_loss: 1.1206\n",
            "Epoch 8/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 127ms/step - accuracy: 0.8034 - loss: 0.8691 - val_accuracy: 0.7970 - val_loss: 1.1016\n",
            "Epoch 9/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.8150 - loss: 0.7951 - val_accuracy: 0.8008 - val_loss: 1.0849\n",
            "Epoch 10/10\n",
            "\u001b[1m141/141\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 131ms/step - accuracy: 0.8275 - loss: 0.7247 - val_accuracy: 0.8057 - val_loss: 1.0690\n",
            "Example 1:\n",
            "English: A group of men are loading cotton onto a truck\n",
            "True French: Un groupe d'hommes chargent du coton dans un camion\n",
            "Predicted: de gens sont assises sur un banc en train de parler\n",
            "========================================\n",
            "Example 2:\n",
            "English: A man sleeping in a green room on a couch.\n",
            "True French: Un homme dormant dans une chambre verte sur un canapé.\n",
            "Predicted: est dans un café dans un parc\n",
            "========================================\n",
            "Example 3:\n",
            "English: A boy wearing headphones sits on a woman's shoulders.\n",
            "True French: Un garçon avec un casque est assis sur les épaules d'une femme.\n",
            "Predicted: avec un chapeau de sécurité est assis sur un banc\n",
            "========================================\n",
            "Example 4:\n",
            "English: Two men setting up a blue ice fishing hut on an iced over lake\n",
            "True French: Deux hommes installant une tente de pêche sur glace bleue sur un lac gelé\n",
            "Predicted: enfants sont assis sur un banc dans un parc dattractions\n",
            "========================================\n",
            "Example 5:\n",
            "English: A balding man wearing a red life jacket is sitting in a small boat.\n",
            "True French: Un homme chauve vêtu d'un gilet de sauvetage rouge est assis dans un petit bateau.\n",
            "Predicted: dun homme portant un chapeau rouge et un chapeau de baseball est assis sur un canapé\n",
            "========================================\n",
            "Example 6:\n",
            "English: A lady in a red coat, holding a bluish hand bag likely of asian descent, jumping off the ground for a snapshot.\n",
            "True French: Une femme en manteau rouge, avec un sac à main bleuté probablement d'origine asiatique, sautant en l'air pour une photo.\n",
            "Predicted: en rouge et un homme en manteau beige se tient à côté dun arbre en briques avec un bâton\n",
            "========================================\n",
            "Example 7:\n",
            "English: A brown dog is running after the black dog.\n",
            "True French: Un chien brun court après le chien noir.\n",
            "Predicted: chien brun dans leau\n",
            "========================================\n",
            "Example 8:\n",
            "English: A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch.\n",
            "True French: Un jeune garçon vêtu d'un maillot des Giants brandit une batte de base-ball face à une balle qui arrive.\n",
            "Predicted: dun jeune garçon en maillot de bain rouge se tient à côté dun arbre\n",
            "========================================\n",
            "Example 9:\n",
            "English: A man in a cluttered office is using the telephone\n",
            "True French: Un homme dans un bureau encombré utilise le téléphone\n",
            "Predicted: dun homme en train de regarder un homme en train de manger\n",
            "========================================\n",
            "Example 10:\n",
            "English: A smiling woman in a peach tank top stands holding a mountain bike\n",
            "True French: Une femme souriante avec un débardeur pêche est debout, tenant un VTT\n",
            "Predicted: dun homme en costume est assis à côté dune voiture de piquenique\n",
            "========================================\n",
            "Average BLEU Score:  0.04534501506498112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4"
      ],
      "metadata": {
        "id": "cFbuJlYS0vSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer hyperparameters\n",
        "d_model = 64\n",
        "num_heads = 2\n",
        "dff= 128\n",
        "num_layers = 2\n",
        "vocab_size = 10000\n",
        "max_len = 40\n",
        "\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, max_length=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Precompute the positional encodings\n",
        "        pe = np.zeros((max_length, d_model))\n",
        "        position = np.arange(0, max_length)[:, np.newaxis]\n",
        "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "        self.pe = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Add positional encoding to input embeddings\n",
        "        return x + self.pe[:, :tf.shape(x)[1], :]\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        # Output linear projection\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Split the last dimension into (num_heads, depth)\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Apply linear projections\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # Split into heads\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        d_k = tf.cast(tf.shape(q)[-1], tf.float32)\n",
        "        scores = tf.matmul(q, k, transpose_b=True) / tf.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "          scores += (mask)\n",
        "\n",
        "        # Attention weights\n",
        "        weights = tf.nn.softmax(scores, axis=-1)\n",
        "        # Apply attention weights to values\n",
        "        output = tf.matmul(weights, v)\n",
        "\n",
        "        # Concatenate heads and project\n",
        "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "        return self.dense(output)\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, is_decoder=False):\n",
        "        super().__init__()\n",
        "        self.is_decoder = is_decoder\n",
        "        # Self-attention\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Cross-attention (only for decoder)\n",
        "        if is_decoder:\n",
        "          self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        # Layer normalizations\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        if is_decoder:\n",
        "            self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        else:\n",
        "            self.layernorm3 = None\n",
        "\n",
        "    def call(self, x, enc_output=None, look_ahead_mask=None, padding_mask=None, training=None):\n",
        "        # Self-attention\n",
        "        attn1 = self.mha1(x, x, x, look_ahead_mask if self.is_decoder else padding_mask)\n",
        "        x = self.layernorm1(x + attn1)\n",
        "\n",
        "        # Cross-attention (decoder only)\n",
        "        if self.is_decoder and enc_output is not None:\n",
        "            attn2 = self.mha2(enc_output, enc_output, x, padding_mask)\n",
        "            x = self.layernorm2(x + attn2)\n",
        "\n",
        "        # FFN\n",
        "        ffn_output = self.ffn(x)\n",
        "        return self.layernorm3(x + ffn_output) if self.is_decoder else self.layernorm2(x + ffn_output)\n",
        "\n",
        "class SimplifiedTransformer(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Input Embeddings + Positional Encoding\n",
        "        self.enc_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.dec_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.encoder_layers = []\n",
        "        for i in range(num_layers):\n",
        "            encoder_layer = TransformerBlock(d_model, num_heads, dff, is_decoder=False)\n",
        "            self.encoder_layers.append(encoder_layer)\n",
        "\n",
        "        # Create decoder layers\n",
        "        self.decoder_layers = []\n",
        "        for i in range(num_layers):\n",
        "            decoder_layer = TransformerBlock(d_model, num_heads, dff, is_decoder=True)\n",
        "            self.decoder_layers.append(decoder_layer)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        inp, tar = inputs\n",
        "\n",
        "        # Encoder\n",
        "        x = self.enc_embedding(inp) * tf.sqrt(tf.cast(d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "        enc_mask = tf.cast(tf.math.equal(inp, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "        # Encoder padding\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, padding_mask=enc_mask, training=training)\n",
        "        enc_output = x\n",
        "\n",
        "        # Decoder\n",
        "        x = self.dec_embedding(tar) * tf.sqrt(tf.cast(d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((tf.shape(tar)[1], tf.shape(tar)[1])), -1, 0)\n",
        "        dec_mask = tf.cast(tf.math.equal(tar, 0), tf.float32)[:, tf.newaxis, tf.newaxis, :]\n",
        "        combined_mask = tf.maximum(dec_mask, look_ahead_mask)\n",
        "\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, enc_output=enc_output, look_ahead_mask=combined_mask,\n",
        "                     padding_mask=enc_mask, training=training)\n",
        "\n",
        "        return self.final_layer(x)\n"
      ],
      "metadata": {
        "id": "t4gf-aBx0wwP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize data\n",
        "tokenizer_en = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer_fr = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer_en.fit_on_texts(train_df[\"en\"])\n",
        "tokenizer_fr.fit_on_texts(train_df[\"fr\"])\n",
        "\n",
        "train_en_seq = pad_sequences(tokenizer_en.texts_to_sequences(train_df[\"en\"]), maxlen=max_len, padding='post')\n",
        "train_fr_seq = pad_sequences(tokenizer_fr.texts_to_sequences(train_df[\"fr\"]), maxlen=max_len, padding='post')\n",
        "\n",
        "# Transformer model\n",
        "transformer = SimplifiedTransformer()\n",
        "\n",
        "def masked_loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = tf.keras.losses.sparse_categorical_crossentropy(real, pred, from_logits=True)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "transformer.compile(optimizer='adam', loss=masked_loss, metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "transformer.fit([train_en_seq, train_fr_seq[:, :-1]], train_fr_seq[:, 1:],\n",
        "                batch_size=64, epochs=8, verbose=1)\n",
        "\n",
        "# Evaluate BLEU\n",
        "def translate(sentence):\n",
        "    seq = pad_sequences(tokenizer_en.texts_to_sequences([sentence]), maxlen=max_len, padding='post')\n",
        "    output = tf.expand_dims([1], 0)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        pred = transformer([seq, output], training=False)\n",
        "        pred_id = tf.cast(tf.argmax(pred[:, -1:, :], axis=-1), tf.int32)\n",
        "        if pred_id == 0:\n",
        "          break\n",
        "        output = tf.concat([output, pred_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0)[1:].numpy()\n",
        "\n",
        "# Calculate BLEU\n",
        "smoothie = SmoothingFunction().method4\n",
        "bleu_scores = []\n",
        "fr_word_index = {v: k for k, v in tokenizer_fr.word_index.items()}\n",
        "\n",
        "for i in range(10):\n",
        "    pred_ids = translate(val_df[\"en\"].iloc[i])\n",
        "\n",
        "    pred_words = []\n",
        "\n",
        "    for token_id in pred_ids:\n",
        "        if token_id != 0:\n",
        "            # Get the word for this ID, or '' if not found\n",
        "            word = fr_word_index.get(token_id, '')\n",
        "            pred_words.append(word)\n",
        "\n",
        "    true_words = val_df[\"fr\"].iloc[i].split()\n",
        "\n",
        "    if pred_words:\n",
        "        bleu_scores.append(sentence_bleu([true_words], pred_words, smoothing_function=smoothie))\n",
        "\n",
        "transformer_bleu = np.mean(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {transformer_bleu:.4f}\")"
      ],
      "metadata": {
        "id": "4DgpGfJa8ncZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ee740c-0588-4ec6-8ff3-c3bf79b5871a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 66ms/step - accuracy: 0.0177 - loss: 7.3083\n",
            "Epoch 2/8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - accuracy: 0.1420 - loss: 3.6167\n",
            "Epoch 3/8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.2263 - loss: 1.6384\n",
            "Epoch 4/8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.2577 - loss: 0.9032\n",
            "Epoch 5/8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.2731 - loss: 0.5721\n",
            "Epoch 6/8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.2818 - loss: 0.3718\n",
            "Epoch 7/8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.2846 - loss: 0.2527\n",
            "Epoch 8/8\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.2888 - loss: 0.1686\n",
            "Average BLEU Score: 0.0015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis"
      ],
      "metadata": {
        "id": "IpOO8fa_bmH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model in Part 4 performs worse than the model in Part 2. Although, the model in Part 4 sholud be performing better than the model in Part 2. It might be doing worse due to the fact that the validation set I used was relatively small. Increasing the size might make it perform better, but my computer takes too long to run this. Furthermore, I noticed that for each epoch, the model in Part 4 completed each epoch significantly faster than the model in part 2."
      ],
      "metadata": {
        "id": "Mqusopokbngx"
      }
    }
  ]
}